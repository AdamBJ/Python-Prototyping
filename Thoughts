There is no "right vs left" distinction when working with integers in Python or any other language. The main tools we have to manipulate integers are 
shifts and binary operations like AND and OR. For shifts (>>, <<), an integer either has its least significant bits truncated (in the case of >>), 
or has 0s added at the least significant position (in the case of <<). Each 0 that is added in this way doubles the original number. 
The way the number is laid out in memory doesn't matter to us. Python takes care of making sure that >> truncates and << doubles regardless of endianness.

We can also manipulate integers by applying binary operations to them, so long as we have an additional binary integer to use as a second operand. 
For example, we can AND two integers together, which produces another integer. In the land of Python, where we work with unbounded integers, these 
binary operations are particularly easy because we don't have to worry about integer lengths mismatches when we're comparing two integers. All 
integers have the same length (infinite!) 

To be absolutely clear, the computer cannot understand instructions like "scan this binary integer from 
right to left", or even "scan this integer from least significant position to most significant". Under the hood, such functions utilize the masking 
and ">>" operations just mentioned to achieve the desired result.

Now, how about reading files? When we read a file in Python (or any other language), we have to pick an "end" at which to start reading the file. 
We could start reading the file from the begining (e.g. at "The" in "The quick red fox jumped over the lazy brown dog") or at the end (i.e. from "dog"). 
Thankfully, the designers of the various "read" functions took a sane approach and decided to read (i.e. stream) files in the order that humans read them. 
That is, the first byte of the file we process corresponds to the first character a human would read ("T" in our example). 

When we look at the binary representation of a file (i.e. as one long binary integer), it *seems* safe to assume that the more significant 
the bit positions a field is stored at, 
the earlier it is read by a call to `read()`. That is, the first field in a CSV file we read is stored in the most significant bit positions of the 
binary integer that results from viewing the file as a number in binary representation. That means that doing a >> shift would truncate bits associated
 with the final field in the file, not the first.

What does all this mean for the transducer? When we read in a CSV file, we process it in "human order", from the first field a human would read in the 
CSV file to the last. Relying on the assumption in the previous paragraph, if we view the file as one long binary integer, this means we process from 
the most significant bit to the least significant bit. The challenge with this approach is that we currently calculate field widths in reverse "human" 
order, i.e. from the least significant bit position (of the integer representation of the file) to the most significant position. We need to resolve 
this conflict before starting working on the Parabix version of the transducer. Here are some options:

a) Process the file in reverse "human" order (from least sig to most sig?) and use the field widths in the order they were calculated
b) Process the file in "human" order (most sig to least?) and use the field widths in reverse order (i.e. the last field width we calculated is used first)

The current approach stores the field widths in a list in "human" order, and reads the file in human order. That is, if the CSV file was 123,abcd field 
widths = [3, 4]. The current 
approach to this is not ideal, because it requires that we insert each of the field widths we calculate at the start of the field widths list (since 
we first calculate the last field with and then move on to earlier ones). This is slow, because it requires that all the existing items in the list 
be shifted over by a single position. Making this approach even sillier is that when it come time to use the field widths in the pdep and pext functions, 
we process the csv bit streams from least significant position to most significant position. This requires us to reverse the field width list before we
 use it.

Another issue is the order the field widths appear in in the field_widths list. In the early Parabix version of the transducer, the field widths stream 
contained widths in "human" order. That is, if the CSV file was abc,12345, field widths was 3, 5. Since we start loading from the base address of the 
field width stream in the main transducer kernel, this means that the first field we process is 3 and the second field is 5. Likewise, in the stream 
representation 
of the file, the base pointer points to the *start* or the file in memory (i.e. to 'a'). This is where Python and Parabix diverge. In Parabix, we have 
a pointer to the start of the file, which makes it easy to process in human order. In Python, we just have an integer that represents the file. 
We don't have a pointer to a particular location 
in the integer (like the first or last bit). All we need to do to align the Python version with the Parbix version is change the calculate field widths 
function so that it 
processes the file in the same order as Parabix does (starting from the file start, not the file end). We can do this if we know the length of the CSV 
file. That way we can create a mask that starts at the start of the file. Another option is to read the file (i.e. call read()) 
inside calculate_field_widths. That way we can rely on `read()` to read from the start for us.

____________________________


Let's put the endianess issue to bed:

Endianess refers to the layout in memory of the bytes that compose a number. Consider 0x0a0b0c0d, which in decimal is 218893066. From elementary school
, we know that the most significant digits are on the left, and things get progressively less significant as we move to the right. In computer terms, the
"most significant byte" is the byte which contains the most significant (i.e. leftmost in the elementary school approach) bit. In the case of our example number, the most significant byte is
0x0a. Following similiar logic, the least significant byte is 0x0d. Following a big-endian scheme, the most significant byte (0a) would appear
in memory first. Put another way, we start "counting" the memory addresses for the number from 0a. It gets stored at, say, byte 0, 0b gets
stored at address 1, and so on. 0d, the least significant byte would get stored at address 4. In this scheme, the significance of the stored
bytes decrease as memory addresses increase. Assuming memory addresses increase from left to right, the big endian scheme has the advantage
 of storing bytes in the order we write them: |0a|0b|0c|0d|.

Following a little-endian scheme, we do the opposite. We store the least significant byte at the lowest memory address, and the most significant
byte at the highest address. This scheme would see 0d stored at address 0 and 0a stored at address 4. With this scheme, signifance increases
as memory addresses increase. Assuming memory addresses increase from right to left, little endian values appear to be stored "backwards". 
However, it does make sense that significance increases as memory addresses increase.

Now, forgot all the endian stuff. It's important, but not for what we're doing. 

Think about integers in an abstract way. Think of them as though they're written on a heavenly chalk board, from most significant digit to
least significant, just like you've always done. Regardless of whether the bytes of an integer are actually stored in a little endian or 
big endian way, the >> operator still truncates less significant bits and the << operator still adds 0s at the least significant bit position.
In reality, these operations may shift things in memory in one direction or another, depending on endianness. *We don't have to worry about this*.
One of the joys of programming in a high-level langauge is not having to worry about nuts and bolts things like endianess. Things just 
behave in about the way you'd expect. 

What this means for the transducer:

When we read() a file, we read it from start to finish, just like we'd read a book. It doesn't matter if the bytes of the file are laid out
in a big or little endian fashion. Think of the file as one big integer, in the same way that a bit stream is just one big integer. 
Fundamentally, the only operations we can use to manipulte the file (or any other integer) is bit shift, and binary operators like AND and OR.
So, read the file, convert it to an integer. Whether the file is processed from start to end or end to start is up to you. If you want to process
it from start to finish, start building the integer from start to finish (a,b,c). If you want to process it from finish to start, build the integer
from finish to start (c,b,a). Up to you. *No endianness calculations required.*

One reason you might want to strongly consider processing from file start to file end is:
a) it's the sane way to do things
b) When Parabix reads a file, it returns a pointer to the start of a file stream. The first element that we load using the pointer is the
first item in the file. It's the same with integer streams (e.g. field width stream). We're given a pointer, and the first thing we load is 
the first field width in our list/stream (e.g. [1, 2, 3] first thing we load is 1).